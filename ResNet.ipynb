{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb72LJV14BV0loQj7lWkJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shvmshri/ResNet/blob/master/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSVA5rUC-qVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xy_Gm49_ro2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def identity_block_bottleneck(X, f, filters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    \n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "  \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path \n",
        "    X = Conv2D(filters= F2,kernel_size=(f,f),strides=(1,1),padding='same',kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X =  Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X =  BatchNormalization(axis = 3)(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st_ws2aRBKnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block_normal(X,f,filters):\n",
        "   \n",
        "   # Retrieve Filters\n",
        "    F1, F2 = filters\n",
        "\n",
        "   # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "   # First component of main path \n",
        "    X = Conv2D(F1, (f, f), strides = (1,1),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "   # Second component of main path \n",
        "    X =  Conv2D(F2, (f, f), strides = (1,1),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "   # Final step\n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pl3hAbdB4ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block_bottleneck(X, f, filters, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path \n",
        "    X =  Conv2D(F2, (f, f), strides = (1,1),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    \n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "   \n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X =  Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e1Nti8l2RrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block_normal(X, f, filters, s = 2):\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (f, f), strides = (1,1),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path \n",
        "    X=ZeroPadding2D((1,1))(X)\n",
        "    X =  Conv2D(F2, (f, f), strides = (s,s), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    \n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(F2, (1, 1), strides = (s,s), kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "   \n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X =  Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYmxvcXJJX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet_bottleneck(input_shape = (32, 32, 3), classes = 10):\n",
        "  \"\"\"\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input=Input(input_shape)\n",
        "    X=X_input\n",
        "    \n",
        "    # Zero-Padding\n",
        "            #  X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(16, (3, 3), strides = (2, 2),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    # X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    # X = Activation('relu')(X)\n",
        "    \n",
        "\n",
        "    # Stage 2\n",
        "            # X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    for i in range(6):\n",
        "       X = identity_block_bottleneck(X, 3, [8, 8, 16])\n",
        "   \n",
        "    # Stage 3\n",
        "    X = convolutional_block_bottleneck(X, f=3,filters =[16,16,32])\n",
        "\n",
        "    # Stage 4 \n",
        "    for i in range(6):\n",
        "       X = identity_block_bottleneck(X, 3, [16, 16, 32])\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block_bottleneck(X, f=3,filters =[32,32,64])\n",
        "\n",
        "    #Stage 6\n",
        "    for i in range(6):\n",
        "       X = identity_block_bottleneck(X, 3, [32, 32, 64])\n",
        "   \n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D(pool_size= (2,2),strides = (2,2))(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet_bottleneck')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nMCnFzZUMf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet_bottleneck()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLOMTJ7zUSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS7BBqDOUXRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d223290c-b315-4224-e820-07208782b9bb"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "x_train_mean = np.mean(x_train, axis=0)\n",
        "x_train -= x_train_mean\n",
        "x_test -= x_train_mean\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(x_train)\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
        "          validation_data=(x_test,y_test), epochs=70)\n",
        "scores= model.evaluate(x_test,y_test,verbose =1)\n",
        "print('Test loss:',scores[0])\n",
        "print('Test accuracy:',scores[1])\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 102s 260ms/step - loss: 2.2475 - accuracy: 0.1935 - val_loss: 1.9988 - val_accuracy: 0.2667\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 1.9593 - accuracy: 0.2810 - val_loss: 1.8206 - val_accuracy: 0.3389\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 1.8504 - accuracy: 0.3180 - val_loss: 1.7168 - val_accuracy: 0.3706\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.7734 - accuracy: 0.3454 - val_loss: 1.6506 - val_accuracy: 0.3910\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.7085 - accuracy: 0.3703 - val_loss: 1.5774 - val_accuracy: 0.4188\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.6528 - accuracy: 0.3923 - val_loss: 1.5448 - val_accuracy: 0.4318\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.6001 - accuracy: 0.4129 - val_loss: 1.5097 - val_accuracy: 0.4508\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 83s 211ms/step - loss: 1.5589 - accuracy: 0.4297 - val_loss: 1.4350 - val_accuracy: 0.4781\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.5220 - accuracy: 0.4438 - val_loss: 1.4000 - val_accuracy: 0.4850\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 1.4811 - accuracy: 0.4606 - val_loss: 1.3645 - val_accuracy: 0.5034\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.4478 - accuracy: 0.4736 - val_loss: 1.3263 - val_accuracy: 0.5163\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.4193 - accuracy: 0.4819 - val_loss: 1.3212 - val_accuracy: 0.5191\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 1.3868 - accuracy: 0.4959 - val_loss: 1.2624 - val_accuracy: 0.5413\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 1.3590 - accuracy: 0.5085 - val_loss: 1.2326 - val_accuracy: 0.5565\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.3355 - accuracy: 0.5163 - val_loss: 1.2212 - val_accuracy: 0.5625\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.3024 - accuracy: 0.5290 - val_loss: 1.1743 - val_accuracy: 0.5757\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.2829 - accuracy: 0.5376 - val_loss: 1.1579 - val_accuracy: 0.5840\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 1.2423 - accuracy: 0.5504 - val_loss: 1.1244 - val_accuracy: 0.5974\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.2258 - accuracy: 0.5612 - val_loss: 1.1226 - val_accuracy: 0.5989\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.1954 - accuracy: 0.5704 - val_loss: 1.0861 - val_accuracy: 0.6098\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 83s 211ms/step - loss: 1.1709 - accuracy: 0.5794 - val_loss: 1.0756 - val_accuracy: 0.6123\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.1577 - accuracy: 0.5849 - val_loss: 1.0512 - val_accuracy: 0.6229\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 1.1378 - accuracy: 0.5915 - val_loss: 1.0478 - val_accuracy: 0.6255\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.1157 - accuracy: 0.5996 - val_loss: 1.0143 - val_accuracy: 0.6350\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.0934 - accuracy: 0.6084 - val_loss: 1.0132 - val_accuracy: 0.6371\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.0816 - accuracy: 0.6128 - val_loss: 0.9925 - val_accuracy: 0.6466\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.0622 - accuracy: 0.6193 - val_loss: 0.9654 - val_accuracy: 0.6569\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 1.0499 - accuracy: 0.6225 - val_loss: 0.9688 - val_accuracy: 0.6549\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 1.0435 - accuracy: 0.6276 - val_loss: 0.9420 - val_accuracy: 0.6647\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 83s 211ms/step - loss: 1.0220 - accuracy: 0.6348 - val_loss: 0.9460 - val_accuracy: 0.6596\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 1.0116 - accuracy: 0.6418 - val_loss: 0.9296 - val_accuracy: 0.6700\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.9993 - accuracy: 0.6426 - val_loss: 0.9048 - val_accuracy: 0.6812\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 0.9917 - accuracy: 0.6492 - val_loss: 0.9031 - val_accuracy: 0.6828\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 0.9750 - accuracy: 0.6525 - val_loss: 0.9040 - val_accuracy: 0.6792\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.9640 - accuracy: 0.6580 - val_loss: 0.8827 - val_accuracy: 0.6934\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.9587 - accuracy: 0.6596 - val_loss: 0.8762 - val_accuracy: 0.6869\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 0.9442 - accuracy: 0.6653 - val_loss: 0.8841 - val_accuracy: 0.6878\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 0.9353 - accuracy: 0.6678 - val_loss: 0.8715 - val_accuracy: 0.6910\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.9303 - accuracy: 0.6684 - val_loss: 0.8563 - val_accuracy: 0.6961\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 83s 212ms/step - loss: 0.9211 - accuracy: 0.6734 - val_loss: 0.8516 - val_accuracy: 0.7018\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.9127 - accuracy: 0.6762 - val_loss: 0.8435 - val_accuracy: 0.7058\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8999 - accuracy: 0.6779 - val_loss: 0.8422 - val_accuracy: 0.7058\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8869 - accuracy: 0.6866 - val_loss: 0.8321 - val_accuracy: 0.7103\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.8800 - accuracy: 0.6891 - val_loss: 0.8254 - val_accuracy: 0.7091\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8747 - accuracy: 0.6921 - val_loss: 0.8152 - val_accuracy: 0.7106\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.8684 - accuracy: 0.6917 - val_loss: 0.8023 - val_accuracy: 0.7180\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 83s 211ms/step - loss: 0.8641 - accuracy: 0.6939 - val_loss: 0.8095 - val_accuracy: 0.7173\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8543 - accuracy: 0.6970 - val_loss: 0.8188 - val_accuracy: 0.7134\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 0.8455 - accuracy: 0.7003 - val_loss: 0.8040 - val_accuracy: 0.7221\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.8351 - accuracy: 0.7041 - val_loss: 0.7946 - val_accuracy: 0.7259\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8251 - accuracy: 0.7049 - val_loss: 0.7747 - val_accuracy: 0.7286\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.8263 - accuracy: 0.7099 - val_loss: 0.7801 - val_accuracy: 0.7242\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.8168 - accuracy: 0.7105 - val_loss: 0.7834 - val_accuracy: 0.7235\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 83s 212ms/step - loss: 0.8114 - accuracy: 0.7111 - val_loss: 0.7624 - val_accuracy: 0.7317\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8015 - accuracy: 0.7186 - val_loss: 0.7643 - val_accuracy: 0.7337\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.8002 - accuracy: 0.7180 - val_loss: 0.7531 - val_accuracy: 0.7410\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7915 - accuracy: 0.7212 - val_loss: 0.7726 - val_accuracy: 0.7294\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.7844 - accuracy: 0.7231 - val_loss: 0.7485 - val_accuracy: 0.7384\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7793 - accuracy: 0.7264 - val_loss: 0.7369 - val_accuracy: 0.7426\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7797 - accuracy: 0.7231 - val_loss: 0.7325 - val_accuracy: 0.7447\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 82s 209ms/step - loss: 0.7637 - accuracy: 0.7334 - val_loss: 0.7260 - val_accuracy: 0.7472\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7679 - accuracy: 0.7306 - val_loss: 0.7206 - val_accuracy: 0.7494\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 83s 211ms/step - loss: 0.7542 - accuracy: 0.7354 - val_loss: 0.7223 - val_accuracy: 0.7504\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.7549 - accuracy: 0.7352 - val_loss: 0.7113 - val_accuracy: 0.7533\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 83s 212ms/step - loss: 0.7475 - accuracy: 0.7365 - val_loss: 0.7124 - val_accuracy: 0.7535\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7429 - accuracy: 0.7396 - val_loss: 0.7091 - val_accuracy: 0.7521\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7390 - accuracy: 0.7386 - val_loss: 0.7101 - val_accuracy: 0.7518\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 81s 208ms/step - loss: 0.7325 - accuracy: 0.7415 - val_loss: 0.7048 - val_accuracy: 0.7533\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 82s 210ms/step - loss: 0.7243 - accuracy: 0.7453 - val_loss: 0.6922 - val_accuracy: 0.7580\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 82s 211ms/step - loss: 0.7212 - accuracy: 0.7464 - val_loss: 0.6920 - val_accuracy: 0.7597\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "Test loss: 0.7445291507720947\n",
            "Test accuracy: 0.7414000034332275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DUZ9oXLz-8",
        "colab_type": "text"
      },
      "source": [
        "model using normal blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B85uC6vaLb6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet_normal(input_shape = (32, 32, 3), classes = 10):\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    X=X_input\n",
        "    X=MaxPooling2D(pool_size=(2,2),padding='same')(X)\n",
        "    # Zero-Padding\n",
        "            #  X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(16, (3, 3), strides = (2, 2),padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "   # X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "   # X = Activation('relu')(X)\n",
        "    \n",
        "\n",
        "    # Stage 2\n",
        "            # X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    for i in range(10):\n",
        "       X = identity_block_normal(X, 3, [16,16])\n",
        "   \n",
        "    # Stage 3\n",
        "    X = convolutional_block_normal(X, f=3,filters =[16,32])\n",
        "\n",
        "    # Stage 4 \n",
        "    for i in range(10):\n",
        "       X = identity_block_normal(X, 3, [32, 32])\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block_normal(X, f=3,filters =[32,64])\n",
        "\n",
        "    #Stage 6\n",
        "    for i in range(10):\n",
        "       X = identity_block_normal(X, 3, [64, 64])\n",
        "   \n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D(pool_size= (2,2),strides = (2,2))(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model2 = Model(inputs = X_input, outputs = X, name='ResNet_bottleneck')\n",
        "\n",
        "    return model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVg3UlF2NVp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = ResNet_normal()\n",
        "model2.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6WF-x4eNqZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74c81872-7988-412a-e95b-37cb11487183"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model2.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
        "          validation_data=(x_test,y_test), epochs=70)\n",
        "scores= model2.evaluate(x_test,y_test,verbose =1)\n",
        "print('Test loss:',scores[0])\n",
        "print('Test accuracy:',scores[1])\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 2.0450 - accuracy: 0.2662 - val_loss: 1.7375 - val_accuracy: 0.3571\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 1.7617 - accuracy: 0.3598 - val_loss: 1.6236 - val_accuracy: 0.4198\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 1.6673 - accuracy: 0.3939 - val_loss: 1.5488 - val_accuracy: 0.4337\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 1.6276 - accuracy: 0.4136 - val_loss: 1.4787 - val_accuracy: 0.4641\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 1.5701 - accuracy: 0.4339 - val_loss: 1.4249 - val_accuracy: 0.4814\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.5237 - accuracy: 0.4518 - val_loss: 1.4062 - val_accuracy: 0.4957\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 1.4914 - accuracy: 0.4611 - val_loss: 1.3620 - val_accuracy: 0.5076\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 1.4481 - accuracy: 0.4786 - val_loss: 1.3261 - val_accuracy: 0.5203\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 1.4306 - accuracy: 0.4843 - val_loss: 1.3019 - val_accuracy: 0.5371\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 1.3988 - accuracy: 0.4992 - val_loss: 1.2971 - val_accuracy: 0.5327\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 1.4054 - accuracy: 0.4937 - val_loss: 1.3392 - val_accuracy: 0.5230\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 1.3527 - accuracy: 0.5162 - val_loss: 1.2143 - val_accuracy: 0.5673\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.3391 - accuracy: 0.5181 - val_loss: 1.2311 - val_accuracy: 0.5569\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 80s 203ms/step - loss: 1.3113 - accuracy: 0.5281 - val_loss: 1.2135 - val_accuracy: 0.5637\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 80s 205ms/step - loss: 1.2849 - accuracy: 0.5375 - val_loss: 1.1877 - val_accuracy: 0.5757\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 1.2761 - accuracy: 0.5441 - val_loss: 1.2146 - val_accuracy: 0.5635\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 1.2507 - accuracy: 0.5515 - val_loss: 1.1377 - val_accuracy: 0.5929\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.2362 - accuracy: 0.5589 - val_loss: 1.1508 - val_accuracy: 0.5898\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 80s 206ms/step - loss: 1.2354 - accuracy: 0.5595 - val_loss: 1.1201 - val_accuracy: 0.6025\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.2113 - accuracy: 0.5682 - val_loss: 1.1218 - val_accuracy: 0.6068\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.1968 - accuracy: 0.5727 - val_loss: 1.1220 - val_accuracy: 0.5942\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.1906 - accuracy: 0.5748 - val_loss: 1.0750 - val_accuracy: 0.6242\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.1723 - accuracy: 0.5827 - val_loss: 1.0734 - val_accuracy: 0.6180\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.1614 - accuracy: 0.5848 - val_loss: 1.0806 - val_accuracy: 0.6157\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.1367 - accuracy: 0.5953 - val_loss: 1.0458 - val_accuracy: 0.6296\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 1.1362 - accuracy: 0.5951 - val_loss: 1.0739 - val_accuracy: 0.6226\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 1.1200 - accuracy: 0.5990 - val_loss: 1.0341 - val_accuracy: 0.6368\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 77s 196ms/step - loss: 1.1284 - accuracy: 0.5965 - val_loss: 1.0722 - val_accuracy: 0.6271\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 77s 198ms/step - loss: 1.0924 - accuracy: 0.6126 - val_loss: 1.0043 - val_accuracy: 0.6472\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.0715 - accuracy: 0.6181 - val_loss: 0.9882 - val_accuracy: 0.6515\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 1.0698 - accuracy: 0.6188 - val_loss: 0.9941 - val_accuracy: 0.6520\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 1.0603 - accuracy: 0.6216 - val_loss: 0.9887 - val_accuracy: 0.6496\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.0537 - accuracy: 0.6235 - val_loss: 0.9958 - val_accuracy: 0.6504\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 1.0499 - accuracy: 0.6269 - val_loss: 0.9543 - val_accuracy: 0.6638\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.0366 - accuracy: 0.6294 - val_loss: 0.9697 - val_accuracy: 0.6542\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 1.0329 - accuracy: 0.6336 - val_loss: 0.9497 - val_accuracy: 0.6648\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 1.0192 - accuracy: 0.6368 - val_loss: 0.9834 - val_accuracy: 0.6581\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 80s 205ms/step - loss: 1.0181 - accuracy: 0.6389 - val_loss: 0.9447 - val_accuracy: 0.6648\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.0137 - accuracy: 0.6417 - val_loss: 0.9760 - val_accuracy: 0.6565\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 1.0183 - accuracy: 0.6377 - val_loss: 0.9242 - val_accuracy: 0.6770\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 78s 201ms/step - loss: 0.9899 - accuracy: 0.6493 - val_loss: 0.9234 - val_accuracy: 0.6780\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 0.9825 - accuracy: 0.6518 - val_loss: 0.9241 - val_accuracy: 0.6772\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 0.9885 - accuracy: 0.6493 - val_loss: 0.9099 - val_accuracy: 0.6790\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.9728 - accuracy: 0.6547 - val_loss: 0.9067 - val_accuracy: 0.6838\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 0.9737 - accuracy: 0.6528 - val_loss: 0.9037 - val_accuracy: 0.6800\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 0.9669 - accuracy: 0.6582 - val_loss: 0.9218 - val_accuracy: 0.6702\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.9639 - accuracy: 0.6571 - val_loss: 0.8940 - val_accuracy: 0.6836\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 0.9454 - accuracy: 0.6655 - val_loss: 0.8984 - val_accuracy: 0.6815\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 81s 207ms/step - loss: 0.9651 - accuracy: 0.6590 - val_loss: 0.8892 - val_accuracy: 0.6850\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 0.9418 - accuracy: 0.6638 - val_loss: 0.9334 - val_accuracy: 0.6768\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 0.9276 - accuracy: 0.6702 - val_loss: 0.8881 - val_accuracy: 0.6872\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 0.9267 - accuracy: 0.6741 - val_loss: 0.8705 - val_accuracy: 0.6965\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 77s 198ms/step - loss: 1.0262 - accuracy: 0.6351 - val_loss: 0.9021 - val_accuracy: 0.6842\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 76s 195ms/step - loss: 0.9506 - accuracy: 0.6649 - val_loss: 0.8857 - val_accuracy: 0.6887\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 77s 197ms/step - loss: 0.9171 - accuracy: 0.6757 - val_loss: 0.8593 - val_accuracy: 0.6927\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 77s 196ms/step - loss: 0.9039 - accuracy: 0.6802 - val_loss: 0.8716 - val_accuracy: 0.6938\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 77s 197ms/step - loss: 0.8989 - accuracy: 0.6795 - val_loss: 0.8706 - val_accuracy: 0.6918\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 77s 197ms/step - loss: 0.9023 - accuracy: 0.6808 - val_loss: 0.8645 - val_accuracy: 0.6957\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.9142 - accuracy: 0.6771 - val_loss: 0.8629 - val_accuracy: 0.6975\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 79s 203ms/step - loss: 0.8835 - accuracy: 0.6849 - val_loss: 0.8673 - val_accuracy: 0.6974\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 80s 205ms/step - loss: 0.8835 - accuracy: 0.6874 - val_loss: 0.8625 - val_accuracy: 0.6960\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 80s 204ms/step - loss: 0.8793 - accuracy: 0.6882 - val_loss: 0.8385 - val_accuracy: 0.7077\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 0.8798 - accuracy: 0.6876 - val_loss: 0.8421 - val_accuracy: 0.7019\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 0.8651 - accuracy: 0.6918 - val_loss: 0.8355 - val_accuracy: 0.7062\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.8644 - accuracy: 0.6967 - val_loss: 0.8458 - val_accuracy: 0.7034\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 0.8642 - accuracy: 0.6940 - val_loss: 0.8329 - val_accuracy: 0.7073\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 77s 198ms/step - loss: 0.8700 - accuracy: 0.6942 - val_loss: 0.8410 - val_accuracy: 0.7026\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.8634 - accuracy: 0.6944 - val_loss: 0.8353 - val_accuracy: 0.7055\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 78s 198ms/step - loss: 0.8501 - accuracy: 0.6993 - val_loss: 0.8425 - val_accuracy: 0.7045\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 77s 198ms/step - loss: 0.8499 - accuracy: 0.6996 - val_loss: 0.8292 - val_accuracy: 0.7119\n",
            "10000/10000 [==============================] - 5s 477us/step\n",
            "Test loss: 0.9103636869430543\n",
            "Test accuracy: 0.6949999928474426\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}